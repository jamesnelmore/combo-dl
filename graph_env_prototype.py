import marimo

__generated_with = "0.11.23"
app = marimo.App(width="full")


@app.cell
def _():
    import marimo as mo
    import numpy as np
    import gymnasium as gym
    from typing import Optional
    from gymnasium.utils.env_checker import check_env

    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    return Optional, PPO, check_env, gym, make_vec_env, mo, np


@app.cell
def _(mo):
    mo.md(
        """
        # Environment design prototype: Inverse Eigenvalue Problem
        - Env inputs: matrix size (eigenvalues autogenerated from some feasibility conditions. Not all episodes will have possible solutions)
        - Observation space: eigenvalues, matrix (stored as `np.ndarray`s)
        - Action space: `size x size` box of entries to toggle, initializes as empty
        - Reward: difference between given eigenvalues and target eigenvalues, normalized

        ## Full Version additions
        - Variable size graph and autogenerated parameter
        - Try initializing randomly
        """
    )
    return


@app.cell
def _(Optional, float64, gym, np, size):
    class InverseEigenvalue(gym.Env):
        def __init__(self, size: int = 5, max_matrix_value: np.float32 = 100):
            super().__init__()
            self.size = size
            self.max_matrix_value = max_matrix_value
            self._matrix = np.zeros((size, size), dtype=np.float32)
            self._max_steps = 10 * (self.size ** 2) # Can modify each entry 10 times
            self._eigenvalues: np.ndarray | None = None
            get_matrix_space = lambda: gym.spaces.Box(
                low=0,
                high=self.max_matrix_value,
                shape=(size, size),
                dtype=float64,
            )

            self.observation_space = gym.spaces.Dict(
                {
                    "matrix": get_matrix_space(),
                    "target eigenvalues": gym.spaces.Box(
                        low=0,
                        high=np.inf,
                        shape=(size,),
                        dtype=float64,
                    ),
                }
            )

            self.action_space = gym.spaces.Dict({
                "index": gym.spaces.MultiDiscrete([size, size]),
                "value": gym.spaces.Box(low=0, high=1, shape=(), dtype=np.float32)
            })

        def _get_obs(self) -> dict:
            return {"matrix": self._matrix, "target_eigenvalues": self._eigenvalues}

        def _get_info(self) -> dict:
            return {}

        def reset(
            self, seed: Optional[int] = None, options: Optional[dict] = None
        ) -> (dict, dict):
            super().reset(seed=seed)

            self._step_count = 0
            self._eigenvalues = self.generate_eigenvalues()
            self._matrix = np.zeros((self.size, self.size), dtype=np.float32)


            obs = self._get_obs()
            info = self._get_info()

            return obs, info

        def step(self, action: dict):
            self.step_count += 1
            i, j = action["index"]
            new_value = action["value"] * self.max_matrix_value

            self._matrix[i, j] = new_value
            self._matrix[j, i] = new_value # Matrix is symmetric to avoid complex eigenvalues

            reward, diff = self._calculate_reward()

            terminated = np.isclose(diff, 0)
            truncated = self.step_count > self._max_steps

            obs = self._get_obs()
            info = self._get_info()

            return obs, reward, terminated, truncated, info

        def _calculate_reward(self) -> (np.float32, np.float32):
            current_eigenvalues = np.linalg.eigvals(self._matrix)
            current_eigenvalues.sort()

            eigval_diff = np.linalg.norm(self.target_eigenvalues - current_eigenvalues) # Consider different norms
            log_transformed_diff = -np.log(1 + eigval_diff) # Ranges from (-inf, 0]
        
            return log_transformed_diff, eigval_diff

        def generate_eigenvalues(self) -> np.ndarray:
            min_eigenvalue = 0
            max_eigenvalue = self.size * self.max_matrix_value # Based on eigenvalue feasibility

            eigenvalues = np.random.uniform(min_eigenvalue, max_eigenvalue, shape=(size,))
            assert eigenvalues.sum() <= size * self.max_matrix_value

            eigenvalues.sort()

            return eigenvalues
        
    # gym.register(
    #     id="gymnasium_env/GridWorld-v0",
    #     entry_point=GridWorldEnv,
    #     max_episode_steps=300,
    # )
    # test_env = gym.make("gymnasium_env/GridWorld-v0")
    # check_env(test_env.unwrapped)
    return (InverseEigenvalue,)


@app.cell
def _(PPO, make_vec_env):
    vec_env = make_vec_env(
        "gymnasium_env/GridWorld-v0",
        n_envs=20,
        render_mode=None,
        env_kwargs={"size": 32},
    )
    model = PPO("MultiInputPolicy", vec_env, device="mps", verbose=1)
    model.learn(total_timesteps=1_000_000)

    obs = vec_env.reset()
    while True:
        action, _states = model.predict(obs)
        (
            obs,
            rewards,
            dones,
            info,
        ) = vec_env.step(action)
    return action, dones, info, model, obs, rewards, vec_env


if __name__ == "__main__":
    app.run()
