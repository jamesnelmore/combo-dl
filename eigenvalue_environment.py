import numpy as np
import gymnasium as gym
from typing import Optional, cast
from gymnasium.utils.env_checker import check_env
import time

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env


"""
# Environment design prototype: Inverse Eigenvalue Problem
- Env inputs: matrix size (eigenvalues autogenerated from some feasibility conditions. Not all episodes will have possible solutions)
- Observation space: eigenvalues, matrix (stored as `np.ndarray`s)
- Action space: `size x size` box of entries to toggle, initializes as empty
- Reward: difference between given eigenvalues and target eigenvalues, normalized

## Full Version additions
- Variable size graph and autogenerated parameter
- Try initializing randomly
"""


class InverseEigenvalue(gym.Env):
    def __init__(self, size: int = 5, max_matrix_value: float = 100.0):
        super().__init__()

        self.size = size
        self.max_matrix_value = max_matrix_value
        self._matrix = np.zeros((size, size), dtype=np.float32)
        self._max_steps = 10 * (self.size**2)  # Can modify each entry 10 times
        self._eigenvalues: np.ndarray | None = None

        self._current_eigenvalues = np.zeros(size, dtype=np.float32)
        self._eigenvalue_diff_cache = 0.0
        self._reward_computation_interval = 1  # Only compute eigenvalues every N steps

        self.observation_space = gym.spaces.Dict(
            {
                "matrix": gym.spaces.Box(
                    low=-self.max_matrix_value,  # Allow negative values
                    high=self.max_matrix_value,
                    shape=(size, size),
                    dtype=np.float32,
                ),
                "target_eigenvalues": gym.spaces.Box(
                    low=-np.inf,  # Allow negative eigenvalues
                    high=np.inf,
                    shape=(size,),
                    dtype=np.float32,
                ),
            }
        )

        # Action encodes [row_index_norm, col_index_norm, value_norm] all in [0, 1]
        # Indices are derived by scaling to [0, size-1] and flooring
        self.action_space = gym.spaces.Box(
            low=0.0, high=1.0, shape=(3,), dtype=np.float32
        )

    def _get_obs(self) -> dict:
        return {"matrix": self._matrix, "target_eigenvalues": self._eigenvalues}

    def _get_info(self) -> dict:
        return {}

    def reset(
        self, seed: Optional[int] = None, options: Optional[dict] = None
    ) -> tuple[dict, dict]:
        """
        return: obs, info
        """
        super().reset(seed=seed)

        self._step_count = 0
        self._eigenvalues = self.generate_eigenvalues()
        self._matrix = np.zeros((self.size, self.size), dtype=np.float32)
        self._eigenvalue_diff_cache = float("inf")

        obs = self._get_obs()
        info = self._get_info()

        return obs, info

    def step(self, action: np.ndarray) -> tuple[dict, float, bool, bool, dict]:
        """
        returns: obs, reward, terminated, truncated, info
        """
        assert action.shape == (3,)
        # Do action
        i = int(np.floor(action[0] * self.size))
        j = int(np.floor(action[1] * self.size))
        # Ensure indices are within bounds
        i = min(i, self.size - 1)
        j = min(j, self.size - 1)

        new_value = (
            action[2] * 2 - 1
        ) * self.max_matrix_value  # Map [0,1] to [-max_value, max_value]

        self._matrix[i, j] = new_value
        self._matrix[j, i] = new_value  # Symmetric for simplicity

        compute_eigenvalues = (
            self._step_count % self._reward_computation_interval == 0
            or self._step_count < 5
        )

        # Only compute expensive eigenvalues periodically or when needed
        if compute_eigenvalues:
            eigval_diff, log_reward = self._calculate_reward()
            self._eigenvalue_diff_cache = eigval_diff
            reward = log_reward
        else:
            eigval_diff = self._eigenvalue_diff_cache
            reward = -0.01  # Small negative reward for each step

        # Episode termination - check every step but only compute eigenvalues when needed
        if compute_eigenvalues:
            terminated = bool(
                np.isclose(eigval_diff, 0, atol=1e-2)
            )  # Slightly looser tolerance
        else:
            terminated = False

        self._step_count += 1
        truncated = self._step_count >= self._max_steps

        obs = self._get_obs()
        info = self._get_info()

        return obs, reward, terminated, truncated, info

    def _calculate_reward(self) -> tuple[float, float]:
        """
        returns: eigenvalue_diff, log_transformed_eigenvalue_diff
        """
        self._current_eigenvalues[:] = np.linalg.eigvals(self._matrix).real
        self._current_eigenvalues.sort()

        assert self._eigenvalues is not None
        eigval_diff = float(
            np.linalg.norm(self._eigenvalues - self._current_eigenvalues)
        )  # Consider different norms
        log_transformed_diff = float(
            -np.log(1.0 + eigval_diff)
        )  # Ranges from (-inf, 0]

        return eigval_diff, log_transformed_diff

    def generate_eigenvalues(self) -> np.ndarray:
        """Generate achievable eigenvalue targets from a random matrix"""
        # Create random symmetric matrix within our constraints
        random_matrix = np.random.uniform(
            -self.max_matrix_value, self.max_matrix_value, (self.size, self.size)
        )
        random_matrix = (random_matrix + random_matrix.T) / 2  # Ensure symmetry

        # Extract its eigenvalues as our target
        eigenvalues = np.linalg.eigvals(random_matrix).real.astype(np.float32)
        eigenvalues.sort()

        return eigenvalues


if __name__ == "__main__":
    vec_env = make_vec_env(
        InverseEigenvalue,
        n_envs=8,  # Reduced from 20 to 8 for faster startup
        env_kwargs={"size": 4},
    )

    # Optimized PPO parameters for faster training
    model = PPO(
        "MultiInputPolicy",
        vec_env,
        device="mps",
        verbose=1,
        learning_rate=3e-3,
        batch_size=128,
        n_epochs=4,
        ent_coef=0.01,
    )

    # Start with smaller number of timesteps to test
    print("Starting training...")
    start_time = time.time()
    model.learn(total_timesteps=100_000)  # Reduced from 1M to 100K for testing
    training_time = time.time() - start_time
    print(f"Training completed in {training_time:.1f} seconds")

    obs = vec_env.reset()
    while True:
        obs_for_predict = cast(dict[str, np.ndarray] | np.ndarray, obs)
        action, _states = model.predict(obs_for_predict)
        (
            obs,
            rewards,
            dones,
            info,
        ) = vec_env.step(action)
