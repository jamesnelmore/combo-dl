# @package hydra.launcher
_target_: hydra_plugins.hydra_submitit_launcher.submitit_launcher.SlurmLauncher

# William & Mary HPC Slurm configuration
# Based on W&M's official documentation:
# https://www.wm.edu/offices/it/services/researchcomputing/using/running_jobs_slurm/

submitit_folder: ${hydra:runtime.output_dir}/.submitit/%j
timeout_min: 240  # 4 hours - adjust based on your job requirements
nodes: 1          # Single node
ntasks_per_node: 1  # Single task per node
cpus_per_task: 8    # 8 cores per task (good for GPU jobs)
mem_gb: 32          # Memory per job - adjust based on requirements

# Additional Slurm parameters for W&M
additional_parameters:
  job-name: ${hydra.job.name}
  output: ${hydra:runtime.output_dir}/slurm-%j.out
  error: ${hydra:runtime.output_dir}/slurm-%j.err
  mail-type: ALL
  mail-user: jnelmore@wm.edu
  # GPU-specific parameters
  gres: "gpu:1"  # Request 1 GPU
  constraint: "hi"  # Use hima cluster for GPU access

# Environment setup for W&M HPC
# W&M uses environment modules and requires sourcing cluster-specific configs
setup:
  - "source /usr/local/etc/sciclone.bashrc"  # Required for W&M main-campus clusters
  - "cd ${hydra:runtime.cwd}"
  - "module load python"  # Load W&M's Python module (check exact name with 'module avail python')
  - "module load cuda"    # Load CUDA module for GPU support
  # Note: uv will handle the virtual environment activation automatically

# Pass environment variables to the job
env:
  PYTHONPATH: ${hydra:runtime.cwd}/src
  HYDRA_FULL_ERROR: "1"
  CUDA_VISIBLE_DEVICES: "0"  # Ensure only one GPU is visible