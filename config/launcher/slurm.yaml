# @package hydra/launcher
_target_: hydra_plugins.hydra_submitit_launcher.submitit_launcher.SubmititLauncher

# William & Mary HPC Slurm configuration
# Based on W&M's official documentation:
# https://www.wm.edu/offices/it/services/researchcomputing/using/running_jobs_slurm/

submitit_folder: ${hydra:runtime.output_dir}/.submitit/%j
timeout_min: 240  # 4 hours - adjust based on your job requirements
cpus_per_task: 8  # W&M clusters have various core counts per node
mem_gb: 32       # Memory per job - adjust based on requirements
gres: gpu:1      # Request 1 GPU (W&M has GPU-enabled nodes)

# W&M doesn't use traditional partitions - jobs are scheduled based on constraints
# Use constraint to specify cluster/node type (e.g., -C jm for james cluster)
# Common W&M constraints: jm (james), ku (kuro), bo (bora), hi (hima)
constraint: hi  # Set based on which W&M cluster you want to use

# Additional Slurm parameters for W&M
additional_parameters:
  job-name: thesis-${hydra.job.name}
  output: ${hydra:runtime.output_dir}/slurm-%j.out
  error: ${hydra:runtime.output_dir}/slurm-%j.err
  mail-type: END,FAIL
  mail-user: jnelmore@wm.edu

# Environment setup for W&M HPC
# W&M uses environment modules and requires sourcing cluster-specific configs
setup:
  - "source /usr/local/etc/sciclone.bashrc"  # Required for W&M main-campus clusters
  - "cd ${hydra:runtime.cwd}"
  - "module load python"  # Load W&M's Python module (check exact name with 'module avail python')
  - "export PYTHONPATH=${hydra:runtime.cwd}/src:$PYTHONPATH"
  # Note: uv will handle the virtual environment activation automatically
