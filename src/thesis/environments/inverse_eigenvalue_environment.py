# ruff: noqa
# pyright: ignore
import time
from typing import cast

import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

"""Environment design prototype: Inverse Eigenvalue Problem.

- Env inputs: matrix size (eigenvalues autogenerated from some feasibility conditions. Not all episodes will have possible solutions)
- Observation space: eigenvalues, matrix (stored as `np.ndarray`s)
- Action space: `size x size` box of entries to toggle, initializes as empty
- Reward: difference between given eigenvalues and target eigenvalues, normalized

## Full Version additions
- Variable size graph and autogenerated parameter
- Try initializing randomly
"""


class InverseEigenvalue(gym.Env):
    def __init__(self, size: int = 5, max_matrix_value: float = 100.0):
        super().__init__()

        self.size = size
        self.max_matrix_value = max_matrix_value
        self._matrix = np.zeros((size, size), dtype=np.bool)
        self._max_steps = 5 * (self.size**2)  # Can modify each entry 5 times
        self._eigenvalues: np.ndarray | None = None

        self._current_eigenvalues = np.zeros(size, dtype=np.float32)
        self._eigenvalue_diff_cache = 0.0
        self._reward_computation_interval = 1  # Only compute eigenvalues every N steps

        self.observation_space = gym.spaces.Dict({
            "matrix": gym.spaces.MultiBinary((size, size)),
            "target_eigenvalues": gym.spaces.Box(
                low=-np.inf,  # Allow negative eigenvalues
                high=np.inf,
                shape=(size,),
                dtype=np.float32,
            ),
        })

        # Action encodes [row_index_norm, col_index_norm, value_norm] all in [0, 1]
        # Indices are derived by scaling to [0, size-1] and flooring
        self.action_space = gym.spaces.Discrete(self.size**2)

    def _get_obs(self) -> dict:
        return {"matrix": self._matrix, "target_eigenvalues": self._eigenvalues}

    def _get_info(self) -> dict:
        return {}

    def reset(self, seed: int | None = None, options: dict | None = None) -> tuple[dict, dict]:
        """return: obs, info"""
        super().reset(seed=seed)

        self._step_count = 0
        self._eigenvalues = self.generate_eigenvalues()
        self._matrix = np.zeros((self.size, self.size), dtype=np.bool)
        self._eigenvalue_diff_cache = float("inf")

        obs = self._get_obs()
        info = self._get_info()

        return obs, info

    def step(self, action: int) -> tuple[dict, float, bool, bool, dict]:
        """returns: obs, reward, terminated, truncated, info"""
        # assert action.shape == (2,)
        # Do action
        i = action // self.size
        j = action % self.size

        self._matrix[i, j] = not self._matrix[i, j]
        self._matrix[j, i] = not self._matrix[j, i]  # Symmetric so eigenvalues are real

        compute_eigenvalues = (
            self._step_count % self._reward_computation_interval == 0 or self._step_count < 5
        )

        # Only compute expensive eigenvalues periodically or when needed
        if compute_eigenvalues:
            eigval_diff, log_reward = self._calculate_reward()
            self._eigenvalue_diff_cache = eigval_diff
            reward = log_reward
        else:
            eigval_diff = self._eigenvalue_diff_cache
            reward = -0.01  # Small negative reward for each step

        # Episode termination - check every step but only compute eigenvalues when needed
        if compute_eigenvalues:
            terminated = bool(np.isclose(eigval_diff, 0, atol=1e-2))  # Slightly looser tolerance
        else:
            terminated = False

        self._step_count += 1
        truncated = self._step_count >= self._max_steps

        obs = self._get_obs()
        info = self._get_info()

        return obs, reward, terminated, truncated, info

    def _calculate_reward(self) -> tuple[float, float]:
        """returns: eigenvalue_diff, log_transformed_eigenvalue_diff"""
        matrix_float = self._matrix.astype(np.float32)
        self._current_eigenvalues[:] = np.linalg.eigvals(matrix_float).real
        self._current_eigenvalues.sort()

        assert self._eigenvalues is not None
        eigval_diff = float(np.linalg.norm(self._eigenvalues - self._current_eigenvalues))

        # Better reward shaping
        log_transformed_diff = float(-np.log(1.0 + eigval_diff))

        return eigval_diff, log_transformed_diff

    def generate_eigenvalues(self) -> np.ndarray:
        """Generate achievable eigenvalue targets from a random matrix"""
        # Create random symmetric boolean matrix
        random_matrix = np.random.randint(0, 2, size=(self.size, self.size)).astype(np.bool_)
        random_matrix = np.logical_or(random_matrix, random_matrix.T)

        # Convert to float for eigenvalue computation
        matrix_float = random_matrix.astype(np.float32)
        eigenvalues = np.linalg.eigvals(matrix_float).real.astype(np.float32)
        eigenvalues.sort()

        return eigenvalues


if __name__ == "__main__":
    vec_env = make_vec_env(
        InverseEigenvalue,
        n_envs=16,
        env_kwargs={"size": 4},
    )

    model = PPO(
        "MultiInputPolicy",
        vec_env,
        device="mps",
        verbose=1,
        learning_rate=3e-3,
        batch_size=128,
        n_epochs=4,
        ent_coef=0.01,
    )

    print("Starting training...")
    start_time = time.time()
    model.learn(total_timesteps=1_000_000)  # Reduced from 1M to 100K for testing
    training_time = time.time() - start_time
    print(f"Training completed in {training_time:.1f} seconds")

    obs = vec_env.reset()
    while True:
        obs_for_predict = cast(dict[str, np.ndarray] | np.ndarray, obs)
        action, _states = model.predict(obs_for_predict)
        (
            obs,
            rewards,
            dones,
            info,
        ) = vec_env.step(action)
